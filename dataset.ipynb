{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "950c8a5b-4e76-4477-86ef-342afd0c2f39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataset import get_training_data, get_validation_data, get_inference_data\n",
    "from spatial_transforms import (Compose, Normalize, Resize, CenterCrop,\n",
    "                                CornerCrop, MultiScaleCornerCrop,\n",
    "                                RandomResizedCrop, RandomHorizontalFlip,\n",
    "                                ToTensor, ScaleValue, ColorJitter,\n",
    "                                PickFirstChannels)\n",
    "from temporal_transforms import (LoopPadding, TemporalRandomCrop,\n",
    "                                 TemporalCenterCrop, TemporalEvenCrop,\n",
    "                                 SlidingWindow, TemporalSubsampling)\n",
    "\n",
    "from utils import Logger, worker_init_fn, get_lr\n",
    "from mean import get_mean_std\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import json\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "62229a7c-172f-4b62-9349-08a821828057",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loading [0/1136]\n",
      "dataset loading [227/1136]\n",
      "dataset loading [454/1136]\n",
      "dataset loading [681/1136]\n",
      "dataset loading [908/1136]\n",
      "dataset loading [1135/1136]\n",
      "dataset loading [0/243]\n",
      "dataset loading [48/243]\n",
      "dataset loading [96/243]\n",
      "dataset loading [144/243]\n",
      "dataset loading [192/243]\n",
      "dataset loading [240/243]\n",
      "dataset loading [0/244]\n",
      "dataset loading [48/244]\n",
      "dataset loading [96/244]\n",
      "dataset loading [144/244]\n",
      "dataset loading [192/244]\n",
      "dataset loading [240/244]\n"
     ]
    }
   ],
   "source": [
    "dataset = 'workoutform'\n",
    "value_scale = 1\n",
    "\n",
    "mean, std = get_mean_std(value_scale, dataset='0.5')\n",
    "\n",
    "normalize = Normalize(mean, std)\n",
    "scale = ScaleValue(value_scale)\n",
    "spatial_transform = Compose([Resize(size=(320, 320), interpolation=Image.BILINEAR),\n",
    "    RandomHorizontalFlip(p=0.5),\n",
    "    ToTensor(),\n",
    "    ScaleValue(1.)\n",
    "])\n",
    "temporal_transform = Compose([TemporalCenterCrop(16)])\n",
    "\n",
    "video_path = Path('data/workoutform_videos/jpg/')\n",
    "\n",
    "for blaat in video_path.iterdir():\n",
    "    assert os.path.exists(blaat), f'{blaat} does not exist'\n",
    "annotation_path = Path('data/workoutform.json')\n",
    "input_type = 'rgb'\n",
    "file_type = 'jpg'\n",
    "train_data = get_training_data(video_path,\n",
    "                               annotation_path,\n",
    "                               dataset, \n",
    "                               input_type, \n",
    "                               file_type,\n",
    "                               spatial_transform,\n",
    "                               temporal_transform)\n",
    "\n",
    "val_data, collate_fn = get_validation_data(video_path, \n",
    "                                  annotation_path,          \n",
    "                                  dataset, \n",
    "                                  input_type, \n",
    "                                  file_type,\n",
    "                                  spatial_transform,\n",
    "                                  temporal_transform)\n",
    "\n",
    "test_data, _ = get_inference_data(video_path, \n",
    "                                  annotation_path,     \n",
    "                                  dataset, \n",
    "                                  input_type, \n",
    "                                  file_type,\n",
    "                                  'test',\n",
    "                                  spatial_transform,\n",
    "                                  temporal_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                           batch_size=128,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=0,\n",
    "                                           pin_memory=True,\n",
    "                                           worker_init_fn=worker_init_fn)\n",
    "\n",
    "# val_loader = torch.utils.data.DataLoader(val_data,\n",
    "#                                         batch_size=128,\n",
    "#                                              shuffle=False,\n",
    "#                                              num_workers=0,\n",
    "#                                              pin_memory=True,\n",
    "#                                              sampler=None,\n",
    "#                                              worker_init_fn=worker_init_fn)\n",
    "\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(test_data,\n",
    "#                                          batch_size=128,\n",
    "#                                          shuffle=False,\n",
    "#                                          num_workers=0,\n",
    "#                                          pin_memory=True,\n",
    "#                                          worker_init_fn=worker_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "989eef4d-4991-4e34-bfed-069cb0ee9917",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 1136\n",
      "torch.Size([128, 3, 16, 320, 320]) torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "print(f'Training dataset size: {len(train_data)}')\n",
    "for xb, yb in train_loader:\n",
    "    print(xb.shape, yb.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "924d4eb4-6f2f-430e-8d81-1dc644a7b26e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scale_to_255(image):\n",
    "    # Ensure float data type for calculations\n",
    "    image = image.astype(float)\n",
    "    \n",
    "    # Get the minimum and maximum pixel values\n",
    "    min_val = np.min(image)\n",
    "    max_val = np.max(image)\n",
    "    \n",
    "    # Scale to 0-255\n",
    "    scaled = (image - min_val) * (255.0 / (max_val - min_val))\n",
    "    \n",
    "    # Convert back to uint8\n",
    "    return scaled.astype(np.uint8)\n",
    "\n",
    "for x, y in train_data:\n",
    "    frames = x.numpy()\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ccd4fd10-a08d-494a-bd39-1fc7bbe2e67e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 16, 320, 320)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "125014c1-004c-4fac-9874-862177dc182d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.        , 0.98039216], dtype=float32)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = frames\n",
    "b = np.transpose(a, (1,2,3,0))\n",
    "b = b[0]\n",
    "\n",
    "np.max(b, axis=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ad994550-dbd6-4404-9b48-6142ca607a0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 2.1271997 ,  2.22365132, -0.77251635],\n",
       "        [ 2.1271997 ,  2.22365132, -0.80605721],\n",
       "        [ 2.1271997 ,  2.23947527, -0.88990936],\n",
       "        ...,\n",
       "        [ 0.02459477,  0.21400961, -0.42033725],\n",
       "        [ 0.05484807,  0.24565751, -0.38679639],\n",
       "        [ 0.06997473,  0.26148146, -0.37002596]],\n",
       "\n",
       "       [[ 2.1271997 ,  2.22365132, -0.78928678],\n",
       "        [ 2.1271997 ,  2.23947527, -0.83959807],\n",
       "        [ 2.1271997 ,  2.23947527, -0.92345022],\n",
       "        ...,\n",
       "        [-0.33844491, -0.16576518, -0.77251635],\n",
       "        [-0.30819161, -0.13411728, -0.75574592],\n",
       "        [-0.29306495, -0.11829333, -0.73897549]],\n",
       "\n",
       "       [[ 2.1271997 ,  2.22365132, -0.83959807],\n",
       "        [ 2.1271997 ,  2.23947527, -0.90667979],\n",
       "        [ 2.1271997 ,  2.23947527, -0.99053195],\n",
       "        ...,\n",
       "        [-0.686358  , -0.54553996, -1.10792499],\n",
       "        [-0.67123135, -0.52971601, -1.09115456],\n",
       "        [-0.65610469, -0.51389207, -1.07438413]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 2.14232635,  2.35024292, -1.39302232],\n",
       "        [ 2.14232635,  2.35024292, -1.39302232],\n",
       "        [ 2.14232635,  2.35024292, -1.39302232],\n",
       "        ...,\n",
       "        [ 2.1271997 ,  2.25529922,  2.01137526],\n",
       "        [ 2.14232635,  2.27112317,  2.04491612],\n",
       "        [ 2.14232635,  2.28694712,  2.06168655]],\n",
       "\n",
       "       [[ 2.14232635,  2.35024292, -1.39302232],\n",
       "        [ 2.14232635,  2.35024292, -1.39302232],\n",
       "        [ 2.14232635,  2.35024292, -1.39302232],\n",
       "        ...,\n",
       "        [ 2.11207304,  2.22365132,  1.9778344 ],\n",
       "        [ 2.1271997 ,  2.25529922,  2.02814569],\n",
       "        [ 2.1271997 ,  2.27112317,  2.04491612]],\n",
       "\n",
       "       [[ 2.14232635,  2.35024292, -1.39302232],\n",
       "        [ 2.14232635,  2.35024292, -1.39302232],\n",
       "        [ 2.14232635,  2.35024292, -1.39302232],\n",
       "        ...,\n",
       "        [ 2.09694639,  2.20782737,  1.96106397],\n",
       "        [ 2.11207304,  2.23947527,  2.01137526],\n",
       "        [ 2.11207304,  2.25529922,  2.02814569]]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = np.array([0.41715325, 0.39402192, 0.35711448])\n",
    "std = np.array([0.25924891, 0.24782488, 0.23383827])\n",
    "c = (b - mean) / std\n",
    "\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a0fd38-f11b-4ced-b5d7-baff613ec14f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "frames = xb.numpy()[0].reshape(16, 112, 112, -1)\n",
    "\n",
    "fig, axes = plt.subplots(len(frames) // 4, 4, figsize=(20, 40))\n",
    "fps = 30.\n",
    "\n",
    "# Plot each frame\n",
    "for i, row in enumerate(axes):\n",
    "    for j, ax in enumerate(row):\n",
    "        frame = i*4 + j\n",
    "        time = frame / fps\n",
    "        title = f'Frame: {frame}'\n",
    "        # if time >= errors[key][0][0] and time <= errors[key][0][1]:\n",
    "        #     title = f'{title}\\nKnees Inward {time:.2f}'\n",
    "\n",
    "        ax.set_title(title)\n",
    "        ax.imshow(cv2.cvtColor(frames[frame], cv2.COLOR_BGR2RGB))\n",
    "        ax.axis('off')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f12e86-b68a-403d-8e4a-23b9b7486166",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaac90a-3492-4585-99a6-a03caabe226a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'Validation dataset size: {len(val_data)}')\n",
    "for xb, yb in val_loader:\n",
    "    print(xb.shape, yb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcecbdd3-936b-4997-9387-feb676cd7e84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'Test dataset size: {len(test_data)}')\n",
    "for xb, yb in test_loader:\n",
    "    print(xb.shape, yb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd77f037-d0f1-461a-8327-792ebee7769c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SupaFakeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = np.random.random((3,8,112,112))\n",
    "        y = random.randint(0, 51)\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d854f4c5-a129-4bd9-9065-81c35703eb84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = SupaFakeDataset(10000)\n",
    "\n",
    "for x, y in train_data:\n",
    "    print(x.shape, y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0466820d-d5de-4a8e-b0f3-4fcf38aa2667",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!df -h /dev/shm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5522270-ecbb-44fb-9c13-e61f1a7584e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ipcs -lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0926f42-d669-487b-88f6-00c3e70ba578",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'Sharing strategy: \"{mp.get_sharing_strategy()}\"')\n",
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                           batch_size=4,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=4,\n",
    "                                           pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d5674d-3eea-435b-b328-5452a20224e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for xb, yb in train_loader:\n",
    "    print(xb.shape, yb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a5acb35-eb88-4111-ad26-c053e28800ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets.videodataset import get_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e0ebc9f-289b-4edd-989f-6bef4663b804",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = Path('data/workoutform.json')\n",
    "with annotation_path.open('r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "video_ids, video_paths, annotations = get_database(data, 'testing', Path('data'), (lambda root_path, label, video_id:\n",
    "                                                           root_path / video_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4179f3c-6cfc-4962-b698-b0e393a2d1da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(video_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33e53ef0-72db-48e5-8bf0-82459b2ff9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kie': 232, 'kfe': 917, 'correct': 474}\n",
      "kie:\t 0.14294516327788045\n",
      "kfe:\t 0.5650030807147258\n",
      "correct:\t 0.2920517560073937\n"
     ]
    }
   ],
   "source": [
    "def get_class_weights(data_path):\n",
    "    with annotation_path.open('r') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    \n",
    "\n",
    "counts = { 'kie': 0, 'kfe': 0, 'correct': 0 }\n",
    "for key, value in data['database'].items():\n",
    "    label = value['annotations']['label']\n",
    "    counts[label] += 1\n",
    "print(counts)\n",
    "total = 0\n",
    "for key, value in counts.items():\n",
    "    total += value\n",
    "\n",
    "for key, value in counts.items():\n",
    "    print(f'{key}:\\t {value/total}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d90b0dcf-f046-4794-ae19-4fe4ae78a763",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1623"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['database'].items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ac25306-8389-4063-a1c8-0e5cfc148319",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14294516 0.56500308 0.29205176]\n",
      "[6.99568966 1.76990185 3.42405063]\n",
      "[0.57390443 0.1451972  0.28089837]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "weights = np.array([value/total for value in counts.values()])\n",
    "print(weights)\n",
    "weights = 1.0 / weights\n",
    "print(weights)\n",
    "weights = weights / np.sum(weights)\n",
    "print(weights)\n",
    "print(np.sum(weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50e4b6d-5227-48f2-9b03-c4279a58aad3",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Compute mean and std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c29edb2b-295c-4df1-a4bb-dd448a03d783",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 102511/189547 [07:42<06:32, 221.54it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m images\n\u001b[1;32m     13\u001b[0m all_paths \u001b[38;5;241m=\u001b[39m glob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/workoutform_videos/jpg/**/*.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_paths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m images_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(result)\n",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m, in \u001b[0;36mprocess_images\u001b[0;34m(paths)\u001b[0m\n\u001b[1;32m      6\u001b[0m images \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m tqdm(paths):\n\u001b[0;32m----> 8\u001b[0m     image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m240\u001b[39m, \u001b[38;5;241m240\u001b[39m)))\n\u001b[1;32m      9\u001b[0m     images\u001b[38;5;241m.\u001b[39mappend(image)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m images\n",
      "File \u001b[0;32m~/venvs/thesis/lib/python3.11/site-packages/PIL/Image.py:3442\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3439\u001b[0m     fp \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO(fp\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m   3440\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 3442\u001b[0m prefix \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m   3444\u001b[0m preinit()\n\u001b[1;32m   3446\u001b[0m warning_messages: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from glob import glob\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "def process_images(paths):\n",
    "    images = []\n",
    "    for path in tqdm(paths):\n",
    "        image = np.array(Image.open(path).resize((240, 240)))\n",
    "        images.append(image)\n",
    "    \n",
    "    return images\n",
    "\n",
    "all_paths = glob('data/workoutform_videos/jpg/**/*.jpg')\n",
    "result = process_images(all_paths)\n",
    "\n",
    "images_np = np.stack(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4988345d-d5da-4f15-a41a-cfab64861525",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images_scaled = images_np / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a32f63-1945-47bd-994b-23fbd5e8433b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('start calculating mean and std')\n",
    "mean = np.mean(images_scaled, axis=(0, 1, 2))  # Mean for each channel\n",
    "print(\"Mean:\", mean)\n",
    "std = np.std(images_scaled, axis=(0, 1, 2))    # Std for each channel\n",
    "print(\"Std:\", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66b9ca2-4dd7-461c-a481-8f05ef40cafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8779764a-df6b-488b-b954-bfab917cf756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "class_weights = torch.tensor([1,2,3])\n",
    "criterion = CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274cda2e-64c1-4445-8cd3-1da21e3c3ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Thesis)",
   "language": "python",
   "name": "py3.11-thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
